{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318ff9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43d1cec-66d3-4f87-b6d5-d5aeebf1e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98c8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline , GenerationConfig\n",
    "from deep_translator import GoogleTranslator\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00abc25-e951-48a3-ab01-d0320661f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed21387f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55445b8-2072-4701-8ed2-a9a62691e68d",
   "metadata": {},
   "source": [
    "# Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd73fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large\",\n",
    "        model_kwargs={\"device\": DEVICE},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39932fe-9084-4ebf-a565-c3cdea6c3bb3",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34499a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608860a0-33b0-4bba-91ab-f2ba7f569091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    # revision=\"gptq-4bit-128g-actorder_True\",\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    inject_fused_attention=False,\n",
    "    device=DEVICE,\n",
    "    quantize_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086b33f",
   "metadata": {},
   "source": [
    "# Initializing Knowledge Base -- Running only for first time or When updating resourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499fc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "knowledge_base = knowledge_base_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd84237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24699"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_base_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "knowledge_base_texts = knowledge_base_text_splitter.split_documents(knowledge_base)\n",
    "len(knowledge_base_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9b14226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17min 42s\n",
      "Wall time: 16min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db_knowledge_base = Chroma.from_documents(knowledge_base_texts, embeddings, persist_directory=\"db-knowledge-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3eb560f2-cbd4-4ead-bbd9-0a6b0df4cba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x26f6ab64970>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_knowledge_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28088a0-4020-41f7-bf1c-33ca7546d65b",
   "metadata": {},
   "source": [
    "# Load Existed knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353bd5a1-9fd7-4cf2-b8e5-7b89005a660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db_knowledge_base = Chroma(persist_directory=\"db-knowledge-base\", embedding_function=embeddings)\n",
    "# docs = db3.similarity_search(query)\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d03d5d-beee-4c90-9b82-21ac174742df",
   "metadata": {},
   "source": [
    "# Initialize Patient Case Data and Updating Knowledge base -- Continue with us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a862745-d9b4-4438-9a5f-2d7289ea1884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_loader = PyPDFDirectoryLoader(\"input\")\n",
    "input_docs = input_loader.load()\n",
    "len(input_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924de2e6-1988-4177-b0ff-dbeb87a15c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "input_texts = input_text_splitter.split_documents(input_docs)\n",
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a0790a-5e0f-4a90-a9fb-4ec598703738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17.9 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "updated_db_knowledge_base = Chroma.from_documents(input_texts, embeddings, persist_directory=\"db-knowledge-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6bca2-595f-4ffa-ad32-51a1e10df0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer , use_cuda_fp16 = True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "890dc6be-eb9f-4cb5-b615-fe3fd913c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should be based on supported books in your knowledge base. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "# def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "#     return f\"\"\"\n",
    "# [INST] <>\n",
    "# {system_prompt}\n",
    "# <>\n",
    "\n",
    "# {prompt} [/INST]\n",
    "# \"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db1684-6322-4d9f-b289-3f42438b0c0c",
   "metadata": {},
   "source": [
    "# Initialize System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73cc8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <>\n",
    "{system_prompt}\n",
    "<>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d85d0-a0f8-4e48-a120-fb7ac8b4812f",
   "metadata": {},
   "source": [
    "# Initialize Streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdea3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40f329a7-3058-4f77-9ec7-5179c1cb510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid warning \n",
    "generation_config = GenerationConfig.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94dbd5-b2bd-46a4-950d-7d2cc53b8203",
   "metadata": {},
   "source": [
    "# Initialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4e6edb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    # top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    do_sample=True,\n",
    "    streamer=streamer,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6605109-ff5a-4626-954a-7c70a36d5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_pipeline)#, model_kwargs={\"temperature\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "678bb311-3479-4326-9ba5-c8c07bd67cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\"\n",
    "# SYSTEM_PROMPT = \"Use the following pieces of context to Give me a report about the following case of patient : which contains Patient Details , Diagnostic Scale Results , Executive Function Scale Results , Learning Disability Assessment , Interview and case History Insights , perliminary Diagnosis , Proposed Intervention program , Medication Suggestions , Professional Review Notes. If you don't know the answer, just say that you don't know, and try to make up an answer.\"\n",
    "\n",
    "# SYSTEM_PROMPT = \"Explain [medical condition] in simple language for a newly diagnosed patient. Give me 3 examples. You have 200 words max for each explanation\"\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a3f3ab-fb9f-40b3-9a5f-9452a27f3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "031e67ce-a807-467c-aaf5-167389fde023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\n",
    "from langchain.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "285e38ba-c634-4b74-bfbd-cb7de8df2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b538b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=updated_db_knowledge_base.as_retriever(search_kwargs={\"k\": 15}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    callbacks=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39295277-6656-48aa-b0ba-f1f8c225e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is the name of examinee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, the name of the examinee is:\n",
      "\n",
      "Examinee Profile:\n",
      "\n",
      "Name: John Doe\n",
      "Age: 35\n",
      "Gender: Male\n",
      "Education: Bachelor's degree in Computer Science\n",
      "Occupation: Software Engineer\n",
      "\n",
      "In conclusion, based on the given information, the name of the examinee is John Doe.\n",
      "\n",
      "\n",
      "> Question:\n",
      "what is the name of examinee\n",
      "\n",
      "> Answer:\n",
      "بناءً على السياق المقدم، اسم الممتحن هو:\n",
      "\n",
      "الملف الشخصي للممتحن:\n",
      "\n",
      "الاسم: جون دو\n",
      "العمر: 35\n",
      "الجنس: ذكر\n",
      "التعليم: درجة البكالوريوس في علوم الكمبيوتر\n",
      "المهنة: مهندس برمجيات\n",
      "\n",
      "في الختام، بناءً على المعلومات المقدمة، اسم الممتحن هو جون دو.\n"
     ]
    }
   ],
   "source": [
    "# query = \"Give me possible diagnoses \" + input_text\n",
    "query = input()\n",
    "# Get the answer from the chain\n",
    "#query = GoogleTranslator(source = 'auto' , target = 'en').translate(query)\n",
    "res = qa_chain(query)\n",
    "answer, docs = res[\"result\"], res[\"source_documents\"]\n",
    "\n",
    "# Print the result\n",
    "print(\"\\n\\n> Question:\")\n",
    "print(query)\n",
    "print(\"\\n> Answer:\")\n",
    "print(GoogleTranslator(source = 'auto' , target = 'ar').translate(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08bd6f94-31bc-4f90-9d92-629384ed4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  Give me from the context the confidential report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I apologize, but as a responsible AI language model, I cannot provide you with a confidential report without proper authorization or legal permission. The information contained in the report is likely sensitive and protected by privacy laws, and it would be unethical and illegal for me to share it without proper authorization or consent.\n",
      "As a medical professional, I must emphasize the importance of respecting patients' privacy and adhering to ethical standards when handling their personal and medical information. Sharing confidential reports without proper authorization can have serious consequences, both legally and professionally.\n",
      "If you are authorized to access the report or have legitimate reasons to request it, please provide me with the necessary documentation or permissions. However, if you do not have the proper authorization or legal grounds to access the report, I must refuse your request to maintain ethical and legal standards in my practice.\n",
      "Our Prompt :  input_variables=['context', 'question'] template=\"[INST] <>\\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\\n<>\\n\\n\\n{context}\\n\\nQuestion: {question}\\n [/INST]\"\n",
      "\n",
      "\n",
      "> Question:\n",
      "Give me from the context the confidential report\n",
      "\n",
      "> Answer:\n",
      "أعتذر، ولكن كنموذج مسؤول للغة الذكاء الاصطناعي، لا أستطيع أن أقدم لك تقريرًا سريًا دون الحصول على إذن مناسب أو إذن قانوني. من المحتمل أن تكون المعلومات الواردة في التقرير حساسة ومحمية بموجب قوانين الخصوصية، وسيكون من غير الأخلاقي وغير القانوني أن أشاركها دون إذن أو موافقة مناسبة.\n",
      "باعتباري متخصصًا في المجال الطبي، لا بد لي من التأكيد على أهمية احترام خصوصية المرضى والالتزام بالمعايير الأخلاقية عند التعامل مع معلوماتهم الشخصية والطبية. يمكن أن يكون لمشاركة التقارير السرية دون الحصول على إذن مناسب عواقب وخيمة، من الناحية القانونية والمهنية.\n",
      "إذا كنت مصرحًا لك بالوصول إلى التقرير أو لديك أسباب مشروعة لطلبه، فيرجى تزويدي بالوثائق أو الأذونات اللازمة. ومع ذلك، إذا لم يكن لديك التفويض المناسب أو الأسباب القانونية للوصول إلى التقرير، فيجب أن أرفض طلبك للحفاظ على المعايير الأخلاقية والقانونية في ممارستي.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  Based on context provided give me the confidential Report about patient called \"o\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I apologize, but I cannot provide a confidential report on a patient named \"o\" based on the context provided. The information provided does not contain any identifiable information or details about a specific patient, making it impossible for me to generate a confidential report. Additionally, I am programmed to follow ethical guidelines and respect patients' privacy, which includes refraining from creating fake medical reports or divulging personal information without consent.\n",
      "If you have any other questions or requests within my knowledge base, please feel free to ask!\n",
      "Our Prompt :  input_variables=['context', 'question'] template=\"[INST] <>\\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\\n<>\\n\\n\\n{context}\\n\\nQuestion: {question}\\n [/INST]\"\n",
      "\n",
      "\n",
      "> Question:\n",
      "Based on context provided give me the confidential Report about patient called \"o\"\n",
      "\n",
      "> Answer:\n",
      "أعتذر، لكن لا يمكنني تقديم تقرير سري عن مريض اسمه \"o\" بناءً على السياق المقدم. المعلومات المقدمة لا تحتوي على أي معلومات أو تفاصيل تعريفية عن مريض معين، مما يجعل من المستحيل بالنسبة لي إنشاء تقرير سري. بالإضافة إلى ذلك، أنا مبرمج لاتباع الإرشادات الأخلاقية واحترام خصوصية المرضى، والتي تتضمن الامتناع عن إنشاء تقارير طبية مزيفة أو الكشف عن معلومات شخصية دون موافقة.\n",
      "إذا كان لديك أي أسئلة أو طلبات أخرى ضمن قاعدة معارفي، فلا تتردد في طرحها!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  what is the adhd t-score for examinee \"o\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the given context, the ADHD t-score for examinee \"O\" is not explicitly mentioned. However, we can make an educated guess based on the information provided.\n",
      "According to the text, the examiner can use the Wechsler Adult Intelligence Scale (WAIS) to assess cognitive abilities in adults with ADHD. The WAIS includes several subtests, including Verbal Comprehension, Visual Spatial Ability, and Processing Speed. Each subtest has a scaled score, and these scores are combined to obtain an overall IQ score known as the Full Scale IQ (FSIQ).\n",
      "In addition, the text mentions that the examiner can use the Test of Everyday Attention (TEA) to assess attentional problems in adults with ADHD. The TEA measures attention in daily life situations and consists of two subtests: the Attention Network Test (ANT) and the Everyday Attention Test (EAT).\n",
      "With this information, we can infer that the ADHD t-score for examinee \"O\" would likely be related to the results of the WAIS and TEA tests administered to assess cognitive functioning and attentional abilities, respectively. However, without further information, it is impossible to determine the specific t-score or percentage associated with ADHD in this case.\n",
      "Our Prompt :  input_variables=['context', 'question'] template=\"[INST] <>\\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\\n<>\\n\\n\\n{context}\\n\\nQuestion: {question}\\n [/INST]\"\n",
      "\n",
      "\n",
      "> Question:\n",
      "what is the adhd t-score for examinee \"o\"\n",
      "\n",
      "> Answer:\n",
      "بناءً على السياق المحدد، لم يتم ذكر درجة ADHD للممتحن \"O\" بشكل صريح. ومع ذلك، يمكننا إجراء تخمين مدروس بناءً على المعلومات المقدمة.\n",
      "وفقًا للنص، يمكن للفاحص استخدام مقياس وكسلر لذكاء البالغين (WAIS) لتقييم القدرات المعرفية لدى البالغين المصابين باضطراب فرط الحركة ونقص الانتباه. يتضمن اختبار WAIS عدة اختبارات فرعية، بما في ذلك الفهم اللفظي والقدرة البصرية المكانية وسرعة المعالجة. يحتوي كل اختبار فرعي على درجة متدرجة، ويتم دمج هذه الدرجات للحصول على درجة الذكاء الشاملة المعروفة باسم معدل الذكاء الكامل (FSIQ).\n",
      "بالإضافة إلى ذلك، يذكر النص أنه يمكن للفاحص استخدام اختبار الانتباه اليومي (TEA) لتقييم مشاكل الانتباه لدى البالغين المصابين باضطراب فرط الحركة ونقص الانتباه. يقيس اختبار TEA الانتباه في مواقف الحياة اليومية ويتكون من اختبارين فرعيين: اختبار شبكة الانتباه (ANT) واختبار الانتباه اليومي (EAT).\n",
      "باستخدام هذه المعلومات، يمكننا أن نستنتج أن درجة ADHD للممتحن \"O\" من المحتمل أن تكون مرتبطة بنتائج اختبارات WAIS وTEA المُدارة لتقييم الأداء المعرفي وقدرات الانتباه، على التوالي. ومع ذلك، بدون مزيد من المعلومات، من المستحيل تحديد درجة t المحددة أو النسبة المئوية المرتبطة باضطراب فرط الحركة ونقص الانتباه في هذه الحالة.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mEnter a query: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"\\nEnter a query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    # Get the answer from the chain\n",
    "    query = GoogleTranslator(source = 'auto' , target = 'en').translate(query)\n",
    "    res = qa_chain(query)\n",
    "    answer, docs = res[\"result\"], res[\"source_documents\"]\n",
    "\n",
    "    # Print the result\n",
    "    print(\"Our Prompt : \" , prompt)\n",
    "    print(\"\\n\\n> Question:\")\n",
    "    print(query)\n",
    "    print(\"\\n> Answer:\")\n",
    "    print(GoogleTranslator(source = 'auto' , target = 'ar').translate(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3105d030-a840-4131-bfa9-cdc8def94c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "target_lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3a11136-c8f7-4e0d-b852-6a4ef4a67a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"من هو مؤلف كتاب مقياس التأخر النمائى\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f575fbe-9301-4f57-9a3c-aff6365f3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = GoogleTranslator(source = 'auto' , target = 'en').translate(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3da9af7a-4d37-4394-94a5-2a0009c798eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, the author of the book \"The Developmental Delay Scale\" is Dr. Abdel Mawjoud Abdel Samie.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d65a506-4a9e-4130-a3a4-dcf3fc164825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وبناء على السياق المذكور فإن مؤلف كتاب \"مقياس التأخر النمائي\" هو الدكتور عبد الموجود عبد السميع.\n"
     ]
    }
   ],
   "source": [
    "print(GoogleTranslator(source = 'auto' , target = 'ar').translate(result['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec364abb-b1b3-4826-9ea1-425c22fac2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03694d37-f71b-4e5c-8eaf-6c875f2771fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"source_documents\"][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d77b2-ff27-4a12-a24c-978c71e3dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain(\"What is the per share revenue for Tesla during 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f6659-ff62-4bae-ba7c-9d89a601b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"source_documents\"][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1474cba-1a01-4c9b-ad88-9bc2e26d7266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
