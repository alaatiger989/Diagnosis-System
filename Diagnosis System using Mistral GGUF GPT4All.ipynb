{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44d0fb4-8f00-4946-a855-2d285cf59fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\n",
    "from langchain.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff751bb-52d5-44dd-bd6e-3e7e64572ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58b5090-e2c6-4aa1-90db-40457028d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline , GenerationConfig\n",
    "from deep_translator import GoogleTranslator\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42aae259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import GPT4All\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6bc2dc9-31a0-4c9b-83de-ea64f6a7bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import ChatCompletionMessage\n",
    "from deep_translator import GoogleTranslator\n",
    "import asyncio\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823305b-dd9d-4734-8e4a-c9fabcf07dc7",
   "metadata": {},
   "source": [
    "# Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c5e71a-4cfd-4b30-ac2f-e3e36df6e5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large\",\n",
    "        model_kwargs={\"device\": DEVICE},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18cea3-8694-4c82-9ffc-76d2395ccc21",
   "metadata": {},
   "source": [
    "# Loading Model (Using Langchain.GPT4All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38c4c8c-015b-49a8-984d-af195032fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496563451600\n"
     ]
    }
   ],
   "source": [
    "llm = GPT4All(\n",
    "    device = 'gpu',\n",
    "    model=\"C:/Users/Alaa AI/Python Projects/Chatbots/GPT by me/alaa_ai_model_mistral_v1.9.gguf\",\n",
    "    max_tokens=2048,\n",
    "    allow_download=False,\n",
    "    backend=\"mistral\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c3d35-c027-4fcb-a6d4-59a3756b6fd9",
   "metadata": {},
   "source": [
    "# Initializing Knowledge Base -- Running only for first time or When updating resourcing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "967d4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "knowledge_base = knowledge_base_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f453e27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24941"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_base_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "knowledge_base_texts = knowledge_base_text_splitter.split_documents(knowledge_base)\n",
    "len(knowledge_base_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d926cfc-2f0f-4aff-a756-d2fafd6437ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# updated_db_knowledge_base = Chroma.from_documents(knowledge_base_texts, embeddings, persist_directory=\"db-knowledge-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e6756-1faf-4d19-86c1-2efc2f73f4bc",
   "metadata": {},
   "source": [
    "# Load existed knowledge base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12835703",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_knowledge_base = Chroma(persist_directory=\"db-knowledge-base\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d4fff-86a4-4082-8420-d321f796cf72",
   "metadata": {},
   "source": [
    "# Initialize Patient Case Data and Updating Knowledge base -- Continue with us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672508c8-1a06-4e57-bf53-71e098d32b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_loader = PyPDFDirectoryLoader(\"input\")\n",
    "input_docs = input_loader.load()\n",
    "len(input_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7360fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily lives and brings them to life in a user-friendly format. The authors discuss many \n",
      "different settings in which ADHD may cause difficulties, including work, school, matters \n",
      "of physical health and well-being, and the issue of excessive use of technology. Although \n",
      "written for consumers, clinicians will find the book to be a clinically useful tool for their \n",
      "adult patients with ADHD, serving as a companion to the newly updated and expanded\n"
     ]
    }
   ],
   "source": [
    "input_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "input_texts = input_text_splitter.split_documents(input_docs)\n",
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964a719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "updated_db_knowledge_base = Chroma.from_documents(input_texts, embeddings, persist_directory=\"db-knowledge-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe08bf2-e500-4a84-98b4-96f4b8a8e909",
   "metadata": {},
   "source": [
    "# Load if the patient is already existed and need more Details about him according to the provided resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f4e2a5-000b-4d1c-814f-b2bd4295bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_db_knowledge_base = Chroma(persist_directory=\"db-knowledge-base\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176201c-739b-4de4-8dfc-b61a08c38fdc",
   "metadata": {},
   "source": [
    "# VectorStore not used because I've used Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3346013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc79356-c134-490e-b1be-ea792d18a262",
   "metadata": {},
   "source": [
    "# Initialize System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2c9c70-1413-488e-9723-38d212b83e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"You are a Definition Rate System . Use the following pieces of context to Give the rate only at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\"\"\"\n",
    "# system_prompt = \"\"\"You are a Definition Rate System, you will use the provided context to rate user Definitions.\n",
    "# Read the given context before answering questions and think step by step. If you can not answer a user question based on \n",
    "# the provided context, inform the user. Do not use any other information for answering user. Provide the rate only to the question.\"\"\"\n",
    "system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
    "Read the given context before answering questions and think step by step. If you can not answer a user question based on \n",
    "the provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4edc256-85cd-4cef-8a83-8fbceeff96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <>\n",
    "{system_prompt}\n",
    "<>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee85adc3-0f9e-4b23-99cb-d21e076ead6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, and try to make up an answer.\"\n",
    "# SYSTEM_PROMPT = \"Use the following pieces of context to Give me a report about the following case of patient : which contains Patient Details , Diagnostic Scale Results , Executive Function Scale Results , Learning Disability Assessment , Interview and case History Insights , perliminary Diagnosis , Proposed Intervention program , Medication Suggestions , Professional Review Notes. If you don't know the answer, just say that you don't know, and try to make up an answer.\"\n",
    "\n",
    "# SYSTEM_PROMPT = \"Explain [medical condition] in simple language for a newly diagnosed patient. Give me 3 examples. You have 200 words max for each explanation\"\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c98ec-341b-43fa-b4b6-d2338395b2d9",
   "metadata": {},
   "source": [
    "# Adding the Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53349627",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me an organized Detailed Form about the Patient Details provided for examinee 'o'\"\n",
    "input_text = GoogleTranslator(source = 'auto' , target = 'en').translate(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef0d96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me an organized Detailed Form about the Patient Details provided for examinee 'o'\n"
     ]
    }
   ],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb612202-a1c0-4f46-90f0-1c1087ad8969",
   "metadata": {},
   "source": [
    "# Initialize the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39859804-4d16-4f66-b483-02f3d041d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee90b8-aa1c-43ac-8793-cf4fa00b71f1",
   "metadata": {},
   "source": [
    "# Use vectorstore if you use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ebca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "# docs = retriever.get_relevant_documents(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6319a6-6014-43ef-b736-66096fb01546",
   "metadata": {},
   "source": [
    "# Initialize The Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349bf90b-4ce0-4d1c-a422-9c7a79f35f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=updated_db_knowledge_base.as_retriever(search_kwargs={\"k\": 15}),\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs={\"prompt\": prompt},\n",
    "#     callbacks=callback_manager,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffdaf410",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdated_db_knowledge_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:100\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[1;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 100\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m load_qa_chain(\n\u001b[0;32m    101\u001b[0m     llm, chain_type\u001b[38;5;241m=\u001b[39mchain_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_chain_type_kwargs\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\langchain\\chains\\question_answering\\__init__.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[1;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m     )\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[0;32m    250\u001b[0m     llm, verbose\u001b[38;5;241m=\u001b[39mverbose, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    251\u001b[0m )\n",
      "File \u001b[1;32mC:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\langchain\\chains\\question_answering\\__init__.py:73\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[1;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_stuff_chain\u001b[39m(\n\u001b[0;32m     64\u001b[0m     llm: BaseLanguageModel,\n\u001b[0;32m     65\u001b[0m     prompt: Optional[BasePromptTemplate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StuffDocumentsChain:\n\u001b[0;32m     72\u001b[0m     _prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m stuff_prompt\u001b[38;5;241m.\u001b[39mPROMPT_SELECTOR\u001b[38;5;241m.\u001b[39mget_prompt(llm)\n\u001b[1;32m---> 73\u001b[0m     llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[0;32m     82\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[0;32m     83\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39mdocument_variable_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     88\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\langchain\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mC:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm, retriever=updated_db_knowledge_base.as_retriever(search_kwargs={\"k\": 3})\n",
    "    ,chain_type= \"stuff\", chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "# qa_chain(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c6dce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 49s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = qa_chain(return_only_outputs = True , \n",
    "   inputs = input_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c032b009-7954-4714-af1c-9dc4fec7933e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n[INST] <>\\nPatient Name: Examinee 'o'\\nDate of Birth: Not Provided\\nGender: Not Provided\\nHeight: Not Provided\\nWeight: Not Provided\\nBlood Pressure: Not Provided\\nTemperature: Not Provided\\nPulse Rate: Not Provided\\nRespiratory Rate: Not Provided\\nAllergies: Not Provided\\nMedications: Not Provided\\nDiagnosis: Not Provided\\nSymptoms: Not Provided\\nFamily History: Not Provided\\nPhysical Examination Findings: Not Provided\\nLab Results: Not Provided\\nImaging Studies: Not Provided\\nProcedures Performed: Not Provided\\nConsultations: Not Provided\\nReferrals: Not Provided\\nAdmission Date: Not Provided\\nDischarge Date: Not Provided\\nLength of Stay: Not Provided\\nReason for Admission: Not Provided\\nReason for Discharge: Not Provided\\nTreatment Plan: Not Provided\\nFollow-up Appointments: Not Provided\\n[/INST]\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09e82c57-af28-4b0a-af14-52e4f086efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?']\n"
     ]
    }
   ],
   "source": [
    "result = res[\"result\"].split(' ')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c4b28dc0-273d-48d1-82d3-d93d86c92147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rate = re.findall(r'\\b\\d+\\b', res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "65a9a96a-99fc-4eb1-919f-480b79ff02b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "adc63a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "<s>[INST] أنت نظام معدل تعريف، وسوف تستخدم السياق المتوفر لتقييم تعريفات المستخدم.\n",
      "اقرأ السياق المحدد قبل الإجابة على الأسئلة وفكر خطوة بخطوة. إذا لم تتمكن من الإجابة على سؤال المستخدم على أساس\n",
      "السياق المقدم، إبلاغ المستخدم. لا تستخدم أي معلومات أخرى للرد على المستخدم. توفير المعدل فقط على السؤال.\n",
      "\n",
      "السياق: الموضوع: الحجاب\n",
      "تعريف الموضوع: عند وفاته\n",
      "المعدل: 0\n",
      "التقييم: صفر\n",
      "\n",
      "الموضوع: الحجاب\n",
      "تعريف الموضوع: قصير\n",
      "المعدل: 1\n",
      "التقييم: واحد\n",
      "\n",
      "الموضوع: الحجاب\n",
      "تعريف الموضوع: ملون\n",
      "المعدل: 2\n",
      "التقييم: اثنان\n",
      "            المستخدم: ما هو معدل \"الحجاب\" عندما يتم تعريفه على أنه \"عند وفاته\"؟ [/إنست]</s>\n",
      "<s>[INST] أنت نظام معدل تعريف، وسوف تستخدم السياق المتوفر لتقييم تعريفات المستخدم.\n",
      "اقرأ السياق المحدد قبل الإجابة على الأسئلة وفكر خطوة بخطوة. إذا لم تتمكن من الإجابة على سؤال المستخدم على أساس\n",
      "السياق المقدم، إبلاغ المستخدم. لا تستخدم أي معلومات أخرى للرد على المستخدم\n"
     ]
    }
   ],
   "source": [
    "print(GoogleTranslator(source = 'auto' , target = 'ar').translate(res[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45af61-9920-4224-9e7f-366e8c898e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a1dcc-41eb-49db-ae5d-9401d69444b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491dfb2-651c-4349-bf1c-8d9b55cf9a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70551887-848c-4e51-8a1c-9c58340b7640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f17daf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"ممكن تكلمنى عن قانون تعليم الأطفال idea 'Respond in details'\",\n",
       " 'result': \"\\n</s>\\n\\nThe Education for Individuals with Difficulties Act 2004, also known as the Improvement of Education for Disabilities with Individuals Act, is a law that focuses on providing education to individuals with difficulties or disabilities. This act covers developmental delay and severe and multiple disabilities. The purpose of this law is to ensure that these children receive an appropriate education tailored to their needs.\\n\\nThe law applies from birth up to the age of 15, and in cases of a severe delay, it may be applied to larger loans. It takes into account that the child's subculture might not encourage long verbal responses, so the focus is on providing an education that caters to their specific needs and abilities.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain(\"what is the figure in the document? Extract the answer from the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e608fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bdcef-3fa2-4620-9181-870192025337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
