{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444c1a4b-b079-451c-b570-a5424fcea8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc960f2-4d9e-4221-af88-e4387a6b49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler\n",
    ")\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37702a30-35f9-4f55-b04e-30353856cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74900de5-51ab-4837-88a0-101bf6e80946",
   "metadata": {},
   "source": [
    "# Load Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f052db2-3fd9-4e0e-8ec3-2d8761775a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\autogptq\\lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# embeddings = HuggingFaceInstructEmbeddings(\n",
    "#         model_name=\"intfloat/multilingual-e5-large\",\n",
    "#         model_kwargs={\"device\": DEVICE},\n",
    "#     )\n",
    "# embedding engine\n",
    "embeddings = HuggingFaceInstructEmbeddings()\n",
    "# embeddings = HuggingFaceInstructEmbeddings(\n",
    "#         model_name=\"hkunlp/instructor-xl\",\n",
    "#         model_kwargs={\"device\": DEVICE},\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a627d-9bf9-4782-8c9d-13ae0da10d0b",
   "metadata": {},
   "source": [
    "# Initialize the Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53b0980-8fdd-4049-b362-3b01bbe3b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c652dd-849a-4783-9057-6f59ea7de64e",
   "metadata": {},
   "source": [
    "# Loading Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434ec2f7-a1d7-4e8a-bd2e-5e85990ec866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    # model_path=\"llama-2-7b-chat.ggmlv3.q2_K.bin\",\n",
    "    model_path=\"C:/Users/Alaa AI/Python Projects/Chatbots/GPT by me/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    n_ctx=40000,\n",
    "    n_gpu_layers=512,\n",
    "    n_batch=30,\n",
    "    callback_manager=callback_manager,\n",
    "    temperature = 0.9,\n",
    "    max_tokens = 30000,\n",
    "    n_parts=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "803e5f39-2e18-4c16-a9a4-fd9334d048d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46cc65-421d-436c-93bf-d003ab7a2853",
   "metadata": {},
   "source": [
    "# Load Existing Knowledge Base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e462fe92-861b-47ed-bb83-d56d48c75963",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_db_knowledge_base = Chroma(persist_directory=\"input\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3dbd8-2388-45ab-ac50-0005f107a1fa",
   "metadata": {},
   "source": [
    "# Create New Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c26ba7d0-2cd4-44b7-9fcc-53ad9150f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "knowledge_base = knowledge_base_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9624ab08-e4a8-49da-8047-e8d8b42f738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_base_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "knowledge_base_texts = knowledge_base_text_splitter.split_documents(knowledge_base)\n",
    "len(knowledge_base_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05db04fb-f155-4084-861d-8243d7126f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "updated_db_knowledge_base = Chroma.from_documents(knowledge_base_texts, embeddings, persist_directory=\"db-knowledge-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420526a1-9d8c-46b8-ab93-b570c1cd9b3e",
   "metadata": {},
   "source": [
    "# Prepare Template using the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b9211d-31e1-435c-94d2-20f5cdea5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the date of birth of the examinee 'o'\"\n",
    "search = updated_db_knowledge_base.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "601dee79-0f82-4d0e-9a6f-fa7044ac84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Context: {context}\n",
    "\n",
    "Based on Context provide me answer for following question\n",
    "Question: {question}\n",
    "\n",
    "Tell me the information about the fact. The answer should be from context only\n",
    "do not use general knowledge to answer the query'''\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template= template)\n",
    "final_prompt = prompt.format(question=query, context=search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e87d979b-ee91-4708-a516-f80bbd032e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on the provided context, we can determine that the examinee's name is \"o\". However, there is no information in the given context regarding the date of birth of the examinee \"o\". Therefore, the answer to the question is \"not found\" or \"unavailable\"."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, we can determine that the examinee\\'s name is \"o\". However, there is no information in the given context regarding the date of birth of the examinee \"o\". Therefore, the answer to the question is \"not found\" or \"unavailable\".'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd390f5-d20b-4ba8-a2ab-f6926cfb7f44",
   "metadata": {},
   "source": [
    "# Using Docx and Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0059ac-d637-4e3a-8117-72f75856a629",
   "metadata": {},
   "source": [
    "Adding File 1 to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c374020-38de-41c0-ba8b-7188dad45e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x1863ef07cd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"C:/Users/Alaa AI/Python Projects/Diagnosis/docx_input/1.docx\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "Chroma.from_documents(knowledge_base_texts, embeddings, persist_directory=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48668ed2-57b6-4561-923f-390f8af0d1b3",
   "metadata": {},
   "source": [
    "Adding File 2 to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97b44c1f-11c0-42e3-97c6-cb5a8e909d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x186320d09a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"C:/Users/Alaa AI/Python Projects/Diagnosis/docx_input/2.docx\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "Chroma.from_documents(knowledge_base_texts, embeddings, persist_directory=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a300cb-0870-4c78-806c-5f7c42fb453a",
   "metadata": {},
   "source": [
    "Adding File 3 to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84c148f9-0bd2-482f-93c0-4ce2e3ad5e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x1863ef07eb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"C:/Users/Alaa AI/Python Projects/Diagnosis/docx_input/3.docx\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "Chroma.from_documents(knowledge_base_texts, embeddings, persist_directory=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b23cb-06b3-4c36-b88e-c6f56ae818ec",
   "metadata": {},
   "source": [
    "Adding File 4 to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd79911f-7547-415d-8cfa-cf36cdf3d22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x18f35f6fb50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"C:/Users/Alaa AI/Python Projects/Diagnosis/docx_input/4.docx\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "Chroma.from_documents(knowledge_base_texts, embeddings, persist_directory=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9898669-d6de-447c-9c44-f14029eeea2c",
   "metadata": {},
   "source": [
    "# Using Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7520b6f-55d7-4690-a265-64f3f694f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"C:/Users/Alaa AI/Python Projects/Diagnosis/docx_input/1.docx\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd738b59-f66b-49c9-aaf0-a02491a69860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# embedding engine\n",
    "hf_embedding = HuggingFaceInstructEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc4cb7a1-7290-4f37-88a5-5020bace6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, hf_embedding)\n",
    "\n",
    "# save embeddings in local directory\n",
    "db.save_local(\"faiss_AiArticle1\")\n",
    "\n",
    "# load from local\n",
    "db = FAISS.load_local(\"faiss_AiArticle1/\", embeddings=hf_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2465800-d3f8-4e0a-9df8-e52dad7bb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the date of birth of the examinee 'o'\"\n",
    "search = db.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d1efde-5b90-479c-bdc1-8e5c1b78a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Context: {context}\n",
    "\n",
    "Based on Context provide me answer for following question\n",
    "Question: {question}\n",
    "\n",
    "Tell me the information about the fact. The answer should be from context only\n",
    "do not use general knowledge to answer the query'''\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template= template)\n",
    "final_prompt = prompt.format(question=query, context=search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb981771-5e79-4743-b17e-eba45d0d070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the provided document, the date of birth of the examinee 'o' is February 2nd, 2016."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" According to the provided document, the date of birth of the examinee 'o' is February 2nd, 2016.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18a258-3cf8-48c4-b9cf-80a6652f794e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
